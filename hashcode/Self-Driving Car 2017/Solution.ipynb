{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "def map2int(ls):\n",
    "    return [int(st) for st in ls]\n",
    "\n",
    "def read_int_list(f):\n",
    "    return map2int(f.readline().split(' '))\n",
    "\n",
    "def cal_distance(x0, y0, x1, y1):\n",
    "    return abs(x1-x0) + abs(y1-y0)\n",
    "\n",
    "def cal_end2start(ride1, ride2):\n",
    "    return cal_distance(ride1['x'], ride1['y'], ride2['a'], ride2['b'])\n",
    "\n",
    "def possible_follow(ride1, ride2, end2start):\n",
    "    earliest_finish = ride1['start_els'] + ride1['distance']\n",
    "    return (earliest_finish + end2start < ride2['critical_start'])\n",
    "\n",
    "def cal_loss_waiting_time(waiting_time):\n",
    "    return waiting_time * coeff_waiting_loss\n",
    "\n",
    "def cal_loss_bonus(distance):\n",
    "    return distance * coeff_bonus_loss\n",
    "\n",
    "def is_possible(ride2, end2start, cur_t):\n",
    "    return (cur_t + end2start < ride2['critical_start'])\n",
    "        \n",
    "def cal_reward(ride1, ride2, cur_t, end2start):\n",
    "    bonus = 0\n",
    "    # Bonus\n",
    "    if cur_t + end2start <= ride2['start_els']:\n",
    "        bonus = B\n",
    "    dist = ride2['distance']\n",
    "    \n",
    "    waiting_time = end2start\n",
    "    if cur_t + end2start < ride2['start_els']:\n",
    "        waiting_time += ride2['start_els'] - (cur_t + end2start)\n",
    "        \n",
    "    loss_waiting = cal_loss_waiting_time(waiting_time)\n",
    "    loss_bonus = cal_loss_bonus(dist)\n",
    "    return (bonus, bonus + dist, waiting_time, dist, bonus + dist - loss_waiting - loss_bonus)\n",
    "\n",
    "#fname = 'b_should_be_easy.in'\n",
    "#fout = 'b_should_be_easy.out'\n",
    "#fname = 'c_no_hurry.in'\n",
    "#fout = 'c_no_hurry.out'\n",
    "fname = 'd_metropolis.in'\n",
    "fout = 'd_metropolis.out'\n",
    "#fname = 'e_high_bonus.in'\n",
    "#fout = 'e_high_bonus.out'        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fname) as f:\n",
    "    R, C, F, N, B, T = read_int_list(f)\n",
    "    rides = []\n",
    "    #Rend2start = np.full((N+1, N+1), -np.inf, dtype=float)   \n",
    "    for i_ride in range(N):\n",
    "        x0, y0, x1, y1, start_els, finish = read_int_list(f)\n",
    "        distance = cal_distance(x0, y0, x1, y1)\n",
    "        rides.append({'a': x0, 'b':y0, 'x':x1, 'y':y1, \n",
    "                      'distance': distance, 'critical_start': finish-distance,\n",
    "                      'start_els':start_els, 'finish':finish})\n",
    "        \n",
    "    # Ride N+1 is the starting point\n",
    "    rides.append({'a':0, 'b':0, 'x':0, 'y':0,\n",
    "                  'distance': 0, 'critical_start': 0,\n",
    "                  'start_els': 0, 'finish': 0})\n",
    "    \n",
    "    # Precalculate rides that can follow R\n",
    "    R_links = {}\n",
    "    End2Start = np.zeros((N+1, N+1), dtype=int)\n",
    "    for i_ride1 in tqdm(range(N)):\n",
    "        ride1 = rides[i_ride1]\n",
    "        R_links[str(i_ride1)] = list()\n",
    "        for i_ride2 in range(N):\n",
    "            if i_ride1 == i_ride2: continue\n",
    "            ride1, ride2 = rides[i_ride1], rides[i_ride2]\n",
    "            end2start = cal_end2start(ride1, ride2)\n",
    "            End2Start[i_ride1, i_ride2] = end2start\n",
    "            if possible_follow(ride1, ride2, end2start):\n",
    "                R_links[str(i_ride1)].append(i_ride2)\n",
    "            #print(len(R_links[i_ride1]))\n",
    "    i_ride1 = N\n",
    "    ride1 = rides[i_ride1]\n",
    "    R_links[str(i_ride1)] = list(range(N))\n",
    "    for i_ride2 in range(N):\n",
    "        ride2 = rides[i_ride2]\n",
    "        End2Start[i_ride1, i_ride2] = cal_end2start(ride1, ride2)\n",
    "        \n",
    "pkfname = fname + 'pk'\n",
    "with open(pkfname, 'wb') as f:\n",
    "    pickle.dump((R_links, End2Start, rides, B, N, F ), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkfname = fname + 'pk'\n",
    "with open(pkfname, 'rb') as f:\n",
    "    R_links, End2Start, rides, B, N, F = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_dist = sum([ride['distance'] for ride in rides])/len(rides)\n",
    "avg_end2start = np.mean(End2Start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep First Search\n",
    "coeff_waiting_loss = (B + avg_dist)/(avg_dist + avg_end2start)\n",
    "coeff_bonus_loss = B/(avg_dist + avg_end2start)\n",
    "coeff_exp = 0.95\n",
    "\n",
    "VclRides = [list() for _ in range(F)]\n",
    "selected_rides = set()\n",
    "selected_rides.add(N)\n",
    "\n",
    "total_t = 0\n",
    "total_reward = 0\n",
    "total_bonus = 0\n",
    "for i_vcl in tqdm(range(F)):\n",
    "    i_cur_ride = N\n",
    "    cur_t = 0\n",
    "    cur_ride = rides[i_cur_ride]    \n",
    "    while True:\n",
    "        max_reward = -np.inf\n",
    "        max_i_ride_follow = None\n",
    "        max_bonus = None\n",
    "        max_real_reward = None\n",
    "        max_waiting_time = None\n",
    "        max_dist = None\n",
    "        max_end2start = None\n",
    "        for i_ride_follow in R_links[str(i_cur_ride)]:\n",
    "            if i_ride_follow in selected_rides: continue\n",
    "            ride_follow = rides[i_ride_follow]\n",
    "            end2start = End2Start[i_cur_ride, i_ride_follow] \n",
    "            if not is_possible(ride_follow, end2start, cur_t): continue\n",
    "            bonus, real_reward, waiting_time, dist, reward = cal_reward(cur_ride, ride_follow, cur_t, end2start)\n",
    "            if reward > max_reward:\n",
    "                max_reward = reward\n",
    "                max_real_reward = real_reward\n",
    "                max_i_ride_follow = i_ride_follow\n",
    "                max_bonus = bonus\n",
    "                max_waiting_time = waiting_time\n",
    "                max_dist = dist\n",
    "        if max_i_ride_follow is None: break\n",
    "        total_reward += max_real_reward\n",
    "        total_bonus += max_bonus\n",
    "        \n",
    "        delta_t = max_waiting_time + max_dist\n",
    "        cur_t += delta_t\n",
    "        total_t += delta_t\n",
    "        \n",
    "        #coeff_waiting_loss = coeff_waiting_loss*coeff_exp + (1-coeff_exp)*(max_real_reward / delta_t)\n",
    "        #coeff_bonus_loss = coeff_bonus_loss*coeff_exp + (1-coeff_exp)*(max_bonus / delta_t)\n",
    "        \n",
    "        coeff_waiting_loss = total_reward / total_t\n",
    "        coeff_bonus_loss = total_bonus / total_t\n",
    "        \n",
    "        i_cur_ride = max_i_ride_follow\n",
    "        cur_ride = rides[i_cur_ride]\n",
    "        \n",
    "        VclRides[i_vcl].append(i_cur_ride)\n",
    "        selected_rides.add(i_cur_ride)\n",
    "\n",
    "print (f'Total Reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fout, 'w') as f:\n",
    "    for i, vcl in enumerate(VclRides):\n",
    "        ride_ls = ' '.join([str(i_ride) for i_ride in vcl])\n",
    "        f.write(f'{len(vcl)} {ride_ls} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 207/350 [00:07<00:04, 28.89it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-028155ac62b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mride_follow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_ride_follow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mend2start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnd2Start\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_cur_ride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_ride_follow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_possible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mride_follow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend2start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mbonus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwaiting_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_ride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mride_follow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend2start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_reward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Bread First Search instead of Deep First Search\n",
    "coeff_waiting_loss = (B + avg_dist)/(avg_dist + avg_end2start)\n",
    "coeff_bonus_loss = B / (avg_dist + avg_end2start)\n",
    "coeff_exp = 0.80\n",
    "\n",
    "VclRides = [list() for _ in range(F)]\n",
    "selected_rides = set()\n",
    "selected_rides.add(N)\n",
    "\n",
    "total_t = 0\n",
    "total_reward = 0\n",
    "total_bonus = 0\n",
    "VclCurrRide = [N for _ in range(F)]\n",
    "VclCurrT = [0 for _ in range(F)]\n",
    "VclStopped = [False for _ in range(F)]\n",
    "\n",
    "while True:\n",
    "    stop = True\n",
    "    for i_vcl in tqdm(range(F)):\n",
    "        if VclStopped[i_vcl]: continue\n",
    "            \n",
    "        i_cur_ride = VclCurrRide[i_vcl]\n",
    "        cur_ride = rides[i_cur_ride]\n",
    "        cur_t = VclCurrT[i_vcl]\n",
    "        \n",
    "        max_reward = -np.inf\n",
    "        max_i_ride_follow = None\n",
    "        max_bonus = None\n",
    "        max_real_reward = None\n",
    "        max_waiting_time = None\n",
    "        max_dist = None\n",
    "        for i_ride_follow in R_links[str(i_cur_ride)]:\n",
    "            if i_ride_follow in selected_rides: continue\n",
    "            ride_follow = rides[i_ride_follow]\n",
    "            end2start = End2Start[i_cur_ride, i_ride_follow] \n",
    "            if not is_possible(ride_follow, end2start, cur_t): continue\n",
    "            bonus, real_reward, waiting_time, dist, reward = cal_reward(cur_ride, ride_follow, cur_t, end2start)\n",
    "            if reward > max_reward:\n",
    "                max_reward = reward\n",
    "                max_real_reward = real_reward\n",
    "                max_i_ride_follow = i_ride_follow\n",
    "                max_bonus = bonus\n",
    "                max_waiting_time = waiting_time\n",
    "                max_dist = dist\n",
    "                \n",
    "        if max_i_ride_follow is None: \n",
    "            VclStopped[i_vcl] = True\n",
    "            continue\n",
    "            \n",
    "        #if max_i_ride_follow == 2433:\n",
    "        #    import pdb; pdb.set_trace()   \n",
    "        stop = False\n",
    "        \n",
    "        total_reward += max_real_reward\n",
    "        total_bonus += max_bonus\n",
    "\n",
    "        delta_t = max_waiting_time + max_dist\n",
    "        \n",
    "                    \n",
    "        cur_t += delta_t\n",
    "        total_t += delta_t\n",
    "            \n",
    "        #coeff_waiting_loss = coeff_waiting_loss*coeff_exp + (1-coeff_exp)*(max_real_reward / delta_t)\n",
    "        #coeff_bonus_loss = coeff_bonus_loss*coeff_exp + (1-coeff_exp)*(max_bonus / delta_t)\n",
    "        coeff_waiting_loss = total_reward / total_t\n",
    "        coeff_bonus_loss = total_bonus / total_t\n",
    "\n",
    "        i_cur_ride = max_i_ride_follow\n",
    "        VclCurrT[i_vcl] = cur_t\n",
    "        VclRides[i_vcl].append(i_cur_ride)\n",
    "        VclCurrRide[i_vcl] = i_cur_ride\n",
    "        selected_rides.add(i_cur_ride)\n",
    "        \n",
    "    if stop: break\n",
    "print (f'Total Reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = 0\n",
    "for i, vcl in enumerate(best_vclRides):\n",
    "    cur_t = 0\n",
    "    i_cur_ride = N\n",
    "    for i_ride in vcl:\n",
    "        begin_cur_t = cur_t\n",
    "        cur_t = cur_t + End2Start[i_cur_ride, i_ride]\n",
    "        ride = rides[i_ride]\n",
    "        if cur_t <= ride['start_els']: \n",
    "            cur_t = ride['start_els']\n",
    "            reward += B\n",
    "        cur_t += ride['distance']\n",
    "        if cur_t >= ride['finish']: \n",
    "            import pdb; pdb.set_trace()\n",
    "            print(\"Invalid!\")\n",
    "        reward += ride['distance']\n",
    "        i_cur_ride = i_ride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:42<00:00,  9.31it/s]\n",
      "100%|██████████| 400/400 [00:23<00:00, 17.04it/s] \n",
      "100%|██████████| 400/400 [00:14<00:00, 27.41it/s] \n",
      "100%|██████████| 400/400 [00:06<00:00, 62.56it/s] \n",
      "100%|██████████| 400/400 [00:01<00:00, 240.54it/s] \n",
      "100%|██████████| 400/400 [00:00<00:00, 1400.76it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 39454.45it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 2979967.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 9952254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(80)\n",
    "# Bread First Search and DFS together\n",
    "def cal_loss_waiting_time(waiting_time):\n",
    "    return waiting_time * vcl_coeff_waiting_loss\n",
    "\n",
    "def cal_loss_bonus(distance):\n",
    "    return distance * vcl_coeff_bonus_loss\n",
    "\n",
    "coeff_waiting_loss = (B + avg_dist)/(avg_dist + avg_end2start)\n",
    "coeff_bonus_loss = B / (avg_dist + avg_end2start)\n",
    "\n",
    "VclRides = [list() for _ in range(F)]\n",
    "\n",
    "selected_rides = set()\n",
    "selected_rides.add(N)\n",
    "\n",
    "total_t = 0\n",
    "total_reward = 0\n",
    "total_bonus = 0\n",
    "VclCurrRide = [N for _ in range(F)]\n",
    "VclCurrT = [0 for _ in range(F)]\n",
    "VclStopped = [False for _ in range(F)]\n",
    "\n",
    "VclWaitingCoeff = [2+2*random.random() for _ in range(F)] #Good for d - (2, 2)\n",
    "VclBonusCoeff = [2+2*random.random() for _ in range(F)]\n",
    "\n",
    "while True:\n",
    "    stop = True\n",
    "    for i_vcl in tqdm(range(F)):\n",
    "        if VclStopped[i_vcl]: continue\n",
    "        n_steps = 500\n",
    "        vcl_coeff_waiting_loss = coeff_waiting_loss #* VclWaitingCoeff[i_vcl]\n",
    "        vcl_coeff_bonus_loss = coeff_bonus_loss #* VclWaitingCoeff[i_vcl]\n",
    "        while n_steps > 0:\n",
    "            i_cur_ride = VclCurrRide[i_vcl]\n",
    "            cur_ride = rides[i_cur_ride]\n",
    "            cur_t = VclCurrT[i_vcl]\n",
    "\n",
    "            max_reward = -np.inf\n",
    "            max_i_ride_follow = None\n",
    "            max_bonus = None\n",
    "            max_real_reward = None\n",
    "            max_waiting_time = None\n",
    "            max_dist = None\n",
    "            for i_ride_follow in R_links[str(i_cur_ride)]:\n",
    "                if i_ride_follow in selected_rides: continue\n",
    "                ride_follow = rides[i_ride_follow]\n",
    "                end2start = End2Start[i_cur_ride, i_ride_follow] \n",
    "                if not is_possible(ride_follow, end2start, cur_t): continue\n",
    "                bonus, real_reward, waiting_time, dist, reward = cal_reward(cur_ride, ride_follow, cur_t, end2start)\n",
    "                #if dist > avg_dist*5:\n",
    "                #    reward = reward*0.5\n",
    "                if reward > max_reward:\n",
    "                    max_reward = reward\n",
    "                    max_real_reward = real_reward\n",
    "                    max_i_ride_follow = i_ride_follow\n",
    "                    max_bonus = bonus\n",
    "                    max_waiting_time = waiting_time\n",
    "                    max_dist = dist\n",
    "\n",
    "            if max_i_ride_follow is None: \n",
    "                VclStopped[i_vcl] = True\n",
    "                break\n",
    "\n",
    "            #if max_i_ride_follow == 2433:\n",
    "            #    import pdb; pdb.set_trace()   \n",
    "            stop = False\n",
    "\n",
    "            total_reward += max_real_reward\n",
    "            total_bonus += max_bonus\n",
    "\n",
    "            delta_t = max_waiting_time + max_dist\n",
    "\n",
    "\n",
    "            cur_t += delta_t\n",
    "            total_t += delta_t\n",
    "\n",
    "            #coeff_waiting_loss = coeff_waiting_loss*coeff_exp + (1-coeff_exp)*(max_real_reward / delta_t)\n",
    "            #coeff_bonus_loss = coeff_bonus_loss*coeff_exp + (1-coeff_exp)*(max_bonus / delta_t)\n",
    "            coeff_waiting_loss = total_reward / total_t\n",
    "            coeff_bonus_loss = total_bonus / total_t\n",
    "\n",
    "            i_cur_ride = max_i_ride_follow\n",
    "            VclCurrT[i_vcl] = cur_t\n",
    "            VclRides[i_vcl].append(i_cur_ride)\n",
    "            VclCurrRide[i_vcl] = i_cur_ride\n",
    "            selected_rides.add(i_cur_ride)\n",
    "            n_steps -= 1\n",
    "        \n",
    "    if stop: break\n",
    "print (f'Total Reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29805650321609556"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
